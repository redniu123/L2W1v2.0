#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
L2W1 å…¨æµç¨‹æ¨ç†ä¸è¯„ä¼°è„šæœ¬ (çœŸå®æ¨¡å¼)

å®Œæ•´é—­ç¯æµ‹è¯•æµæ°´çº¿ï¼š
1. è¯»å– Baseline ç»“æœ (Agent A çš„è¯†åˆ«è¾“å‡º)
2. UncertaintyRouter è¿›è¡Œè·¯ç”±å†³ç­–
3. å¯¹å›°éš¾æ ·æœ¬è°ƒç”¨ Agent B (Qwen2.5-VL) è¿›è¡Œè¾¹ç¼˜æ‰«æçº é”™
4. è®¡ç®— CER æ”¹è¿›å’Œ VLM è°ƒç”¨æˆæœ¬

å…³é”®æŒ‡æ ‡:
- CER_improvement = CER_baseline - CER_l2w1
- VLM Call Rate = VLMè°ƒç”¨æ¬¡æ•° / æ€»æ ·æœ¬æ•°
- è¾¹ç•Œä¿®å¤ç‡ = æˆåŠŸä¿®å¤çš„è¾¹ç•Œé”™è¯¯æ•° / æ€»è¾¹ç•Œé”™è¯¯æ•°

Usage:
    python scripts/run_l2w1_pipeline.py --limit 200
    python scripts/run_l2w1_pipeline.py --input results/baseline_results.jsonl
    python scripts/run_l2w1_pipeline.py --agent_b_model Qwen/Qwen2.5-VL-3B-Instruct
"""

import json
import argparse
import sys
import time
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ==================== æ¨¡å—å¯¼å…¥ ====================

# Router æ¨¡å—
try:
    from modules.router.uncertainty_router import (
        UncertaintyRouter,
        RouterConfig,
        RoutingResult,
    )

    HAS_ROUTER = True
except ImportError as e:
    HAS_ROUTER = False
    print(f"[ERROR] Router æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("[INFO] è¯·ç¡®ä¿ modules/router/uncertainty_router.py å­˜åœ¨")

# Agent B æ¨¡å—
try:
    from modules.vlm_expert import (
        AgentBExpert,
        AgentBConfig,
        EIPPromptTemplate,
    )

    HAS_AGENT_B = True
except ImportError as e:
    HAS_AGENT_B = False
    print(f"[ERROR] Agent B æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("[INFO] è¯·ç¡®ä¿å·²å®‰è£…: pip install transformers accelerate bitsandbytes")

# V-CoT Prompter æ¨¡å—
try:
    from modules.vlm_expert.v_cot_prompter import (
        VCoTPrompter,
        VCoTPromptConfig,
        BoundaryRiskType,
    )

    HAS_PROMPTER = True
except ImportError:
    HAS_PROMPTER = False

# ç¬¬ä¸‰æ–¹åº“
try:
    import Levenshtein

    HAS_LEVENSHTEIN = True
except ImportError:
    HAS_LEVENSHTEIN = False
    print("[WARNING] Levenshtein æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å†…ç½®ç®—æ³•")


# ==================== æ•°æ®ç»“æ„å®šä¹‰ ====================


@dataclass
class SampleResult:
    """å•ä¸ªæ ·æœ¬çš„å¤„ç†ç»“æœ"""

    id: str
    image_path: str
    gt_text: str
    agent_a_text: str  # Baseline é¢„æµ‹
    final_text: str  # æœ€ç»ˆè¾“å‡º

    # CER æŒ‡æ ‡
    original_cer: float
    final_cer: float
    cer_improvement: float

    # è·¯ç”±ä¿¡æ¯
    is_vlm_used: bool
    routing_decision: str
    boundary_risk: bool
    boundary_reason: str

    # Agent B ä¿¡æ¯
    agent_b_text: Optional[str] = None
    agent_b_latency_ms: float = 0.0

    # ç½®ä¿¡åº¦ä¿¡æ¯
    avg_confidence: float = 0.0
    left_boundary_confidence: float = 1.0
    right_boundary_confidence: float = 1.0


@dataclass
class PipelineStats:
    """æµæ°´çº¿ç»Ÿè®¡"""

    total_samples: int = 0
    processed_samples: int = 0
    vlm_called_samples: int = 0

    # CER ç»Ÿè®¡
    total_baseline_cer: float = 0.0
    total_final_cer: float = 0.0
    total_cer_improvement: float = 0.0

    # è¾¹ç•Œä¿®å¤ç»Ÿè®¡
    boundary_risk_samples: int = 0
    boundary_fixed_samples: int = 0

    # æ—¶é—´ç»Ÿè®¡
    total_time_ms: float = 0.0
    vlm_total_time_ms: float = 0.0

    # é£é™©ç­‰çº§åˆ†å¸ƒ
    risk_distribution: Dict[str, int] = field(
        default_factory=lambda: {"low": 0, "medium": 0, "high": 0, "critical": 0}
    )

    @property
    def vlm_call_rate(self) -> float:
        if self.processed_samples == 0:
            return 0.0
        return self.vlm_called_samples / self.processed_samples

    @property
    def avg_baseline_cer(self) -> float:
        if self.processed_samples == 0:
            return 0.0
        return self.total_baseline_cer / self.processed_samples

    @property
    def avg_final_cer(self) -> float:
        if self.processed_samples == 0:
            return 0.0
        return self.total_final_cer / self.processed_samples

    @property
    def avg_cer_improvement(self) -> float:
        if self.processed_samples == 0:
            return 0.0
        return self.total_cer_improvement / self.processed_samples

    @property
    def boundary_fix_rate(self) -> float:
        if self.boundary_risk_samples == 0:
            return 0.0
        return self.boundary_fixed_samples / self.boundary_risk_samples


# ==================== è¾…åŠ©å‡½æ•° ====================


def calculate_cer(gt_text: str, pred_text: str) -> float:
    """è®¡ç®—å­—ç¬¦é”™è¯¯ç‡"""
    if len(gt_text) == 0:
        return 0.0 if len(pred_text) == 0 else 1.0

    if HAS_LEVENSHTEIN:
        edit_distance = Levenshtein.distance(gt_text, pred_text)
    else:
        m, n = len(gt_text), len(pred_text)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if gt_text[i - 1] == pred_text[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1]
                else:
                    dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1
        edit_distance = dp[m][n]

    return edit_distance / len(gt_text)


def load_baseline_results(input_path: Path, limit: int = None) -> List[Dict]:
    """åŠ è½½ Baseline ç»“æœ"""
    if not input_path.exists():
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")

    samples = []
    with open(input_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            if limit and len(samples) >= limit:
                break

            line = line.strip()
            if not line:
                continue

            try:
                sample = json.loads(line)
                samples.append(sample)
            except json.JSONDecodeError as e:
                print(f"[WARNING] ç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥: {e}")

    print(
        f"[INFO] åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬" + (f" (é™åˆ¶: {limit})" if limit else "")
    )
    return samples


# ==================== L2W1 æµæ°´çº¿ ====================


class L2W1Pipeline:
    """
    L2W1 å®Œæ•´æ¨ç†æµæ°´çº¿ (çœŸå®æ¨¡å¼)

    æ ¸å¿ƒç»„ä»¶:
    - Router: ä¸ç¡®å®šæ€§è·¯ç”±å™¨ (è¾¹ç•Œæ•æ„Ÿæ£€æµ‹)
    - Agent B: Qwen2.5-VL (4-bit é‡åŒ–)
    """

    def __init__(
        self,
        router_config: RouterConfig = None,
        agent_b_model_path: str = "Qwen/Qwen2.5-VL-3B-Instruct",
        use_4bit: bool = True,
        use_flash_attention: bool = True,
        image_root: str = "",
    ):
        self.image_root = Path(image_root) if image_root else None

        # åˆå§‹åŒ– Router
        self._init_router(router_config)

        # åˆå§‹åŒ– Agent B (çœŸå® VLM)
        self._init_agent_b(agent_b_model_path, use_4bit, use_flash_attention)

        # åˆå§‹åŒ– Prompter
        self._init_prompter()

        # ç»Ÿè®¡
        self.stats = PipelineStats()

    def _init_router(self, config: RouterConfig = None):
        """åˆå§‹åŒ–è·¯ç”±å™¨"""
        if HAS_ROUTER:
            self.router = UncertaintyRouter(config=config or RouterConfig())
            print("[INFO] Router åˆå§‹åŒ–æˆåŠŸ")
        else:
            self.router = None
            print("[WARNING] Router æœªåˆå§‹åŒ–ï¼Œå°†ä½¿ç”¨ç®€åŒ–è·¯ç”±é€»è¾‘")

    def _init_agent_b(
        self,
        model_path: str,
        use_4bit: bool,
        use_flash_attention: bool,
    ):
        """åˆå§‹åŒ– Agent B (Qwen2.5-VL)"""
        if not HAS_AGENT_B:
            raise RuntimeError(
                "Agent B æ¨¡å—ä¸å¯ç”¨ã€‚è¯·å®‰è£…ä¾èµ–:\n"
                "pip install transformers accelerate bitsandbytes\n"
                "pip install flash-attn --no-build-isolation"
            )

        print("=" * 60)
        print("åˆå§‹åŒ– Agent B (Qwen2.5-VL)")
        print("=" * 60)
        print(f"  æ¨¡å‹: {model_path}")
        print(f"  4-bit é‡åŒ–: {use_4bit}")
        print(f"  Flash Attention: {use_flash_attention}")

        config = AgentBConfig(
            model_path=model_path,
            use_4bit=use_4bit,
            use_flash_attention=use_flash_attention,
        )

        self.agent_b = AgentBExpert(config=config, lazy_init=False)
        print("[INFO] Agent B åˆå§‹åŒ–æˆåŠŸ")

    def _init_prompter(self):
        """åˆå§‹åŒ– V-CoT æç¤ºè¯ç”Ÿæˆå™¨"""
        if HAS_PROMPTER:
            self.prompter = VCoTPrompter(VCoTPromptConfig())
            print("[INFO] V-CoT Prompter åˆå§‹åŒ–æˆåŠŸ")
        else:
            self.prompter = None

    def _get_image_path(self, sample: Dict) -> str:
        """è§£æå›¾åƒè·¯å¾„"""
        image_path = sample.get("image_path", "")
        if not image_path:
            return ""

        # æ£€æŸ¥æ˜¯å¦ä¸ºç»å¯¹è·¯å¾„
        if Path(image_path).is_absolute():
            return image_path

        # å°è¯•ç›¸å¯¹äº image_root
        if self.image_root:
            full_path = self.image_root / image_path
            if full_path.exists():
                return str(full_path)

        # å°è¯•ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        full_path = project_root / image_path
        if full_path.exists():
            return str(full_path)

        # ç›´æ¥è¿”å›åŸå§‹è·¯å¾„
        return image_path

    def _get_routing_decision(
        self, sample: Dict
    ) -> Tuple[bool, str, bool, str, float, float]:
        """è·å–è·¯ç”±å†³ç­–"""
        pred_text = sample.get("pred_text", "")
        avg_conf = sample.get("avg_confidence", 1.0)
        char_confidences = sample.get("char_confidences", [])

        if self.router is None:
            # ç®€åŒ–è·¯ç”±é€»è¾‘
            left_conf = 1.0
            right_conf = 1.0
            if char_confidences:
                left_chars = char_confidences[:2]
                right_chars = char_confidences[-2:]
                if left_chars:
                    left_conf = sum(c.get("score", 1.0) for c in left_chars) / len(
                        left_chars
                    )
                if right_chars:
                    right_conf = sum(c.get("score", 1.0) for c in right_chars) / len(
                        right_chars
                    )

            boundary_risk = left_conf < 0.8 or right_conf < 0.8
            boundary_reason = ""
            if left_conf < 0.8:
                boundary_reason += f"å·¦è¾¹ç•Œç½®ä¿¡åº¦ä½({left_conf:.2f}); "
            if right_conf < 0.8:
                boundary_reason += f"å³è¾¹ç•Œç½®ä¿¡åº¦ä½({right_conf:.2f})"

            if avg_conf < 0.6 or (left_conf < 0.6 and right_conf < 0.6):
                is_hard = True
                risk_level = "high"
            elif avg_conf < 0.75 or boundary_risk:
                is_hard = True
                risk_level = "medium"
            else:
                is_hard = False
                risk_level = "low"

            return (
                is_hard,
                risk_level,
                boundary_risk,
                boundary_reason,
                left_conf,
                right_conf,
            )

        # ä½¿ç”¨ Router çš„è¾¹ç•Œæ•æ„Ÿæ£€æµ‹
        (
            boundary_risk,
            boundary_reason,
            left_conf,
            right_conf,
            aspect_ratio,
            char_density,
        ) = self.router.check_boundary_sensitivity(
            text=pred_text, char_confidences=char_confidences, image_size=None
        )

        if boundary_risk:
            is_hard = True
            risk_level = "high" if avg_conf < 0.7 else "medium"
        elif avg_conf < 0.7:
            is_hard = True
            risk_level = "medium"
        else:
            is_hard = False
            risk_level = "low"

        return (
            is_hard,
            risk_level,
            boundary_risk,
            boundary_reason,
            left_conf,
            right_conf,
        )

    def _build_agent_b_prompt(
        self,
        pred_text: str,
        left_conf: float,
        right_conf: float,
        boundary_risk: bool,
    ) -> str:
        """æ„å»º Agent B æç¤ºè¯"""
        if self.prompter and boundary_risk:
            # ä½¿ç”¨ V-CoT è¾¹ç•Œè¡¥å…¨æç¤ºè¯
            risk_type = self.prompter.detect_boundary_risk_type(
                left_confidence=left_conf, right_confidence=right_conf
            )
            prompt = self.prompter.build_boundary_completion_prompt(
                pred_text=pred_text,
                risk_type=risk_type,
                left_confidence=left_conf,
                right_confidence=right_conf,
            )
            return prompt
        else:
            # ä½¿ç”¨æ ‡å‡† EIP æç¤ºè¯
            return EIPPromptTemplate.build_prompt(ocr_text=pred_text, risk_level="high")

    def _call_agent_b(
        self,
        image_path: str,
        pred_text: str,
        left_conf: float,
        right_conf: float,
        boundary_risk: bool,
    ) -> Tuple[str, float]:
        """è°ƒç”¨ Agent B è¿›è¡Œçº é”™"""
        start_time = time.time()

        # æ„å»º Manifest
        manifest = {
            "ocr_text": pred_text,
            "suspicious_index": -1,  # è¾¹ç•Œé—®é¢˜ï¼Œä¸æŒ‡å®šå…·ä½“ä½ç½®
            "suspicious_char": "",
            "risk_level": "high",
            "boundary_risk": boundary_risk,
            "left_boundary_confidence": left_conf,
            "right_boundary_confidence": right_conf,
        }

        # è°ƒç”¨ Agent B
        try:
            result = self.agent_b.process_hard_sample(
                image=image_path, manifest=manifest
            )
            corrected_text = result.get("corrected_text", pred_text)
        except Exception as e:
            print(f"[ERROR] Agent B æ¨ç†å¤±è´¥: {e}")
            corrected_text = pred_text

        latency_ms = (time.time() - start_time) * 1000
        return corrected_text, latency_ms

    def process_sample(self, sample: Dict) -> SampleResult:
        """å¤„ç†å•ä¸ªæ ·æœ¬"""
        sample_id = sample.get("id", "unknown")
        image_path = self._get_image_path(sample)
        gt_text = sample.get("gt_text", "")
        pred_text = sample.get("pred_text", "")
        avg_conf = sample.get("avg_confidence", 0.0)
        original_cer = sample.get("cer", calculate_cer(gt_text, pred_text))

        # è·å–è·¯ç”±å†³ç­–
        (
            is_hard,
            risk_level,
            boundary_risk,
            boundary_reason,
            left_conf,
            right_conf,
        ) = self._get_routing_decision(sample)

        # åˆå§‹åŒ–ç»“æœ
        final_text = pred_text
        agent_b_text = None
        agent_b_latency = 0.0
        is_vlm_used = False

        # å¦‚æœæ˜¯å›°éš¾æ ·æœ¬ï¼Œè°ƒç”¨ Agent B
        if is_hard:
            agent_b_text, agent_b_latency = self._call_agent_b(
                image_path=image_path,
                pred_text=pred_text,
                left_conf=left_conf,
                right_conf=right_conf,
                boundary_risk=boundary_risk,
            )
            final_text = agent_b_text
            is_vlm_used = True

        # è®¡ç®—æœ€ç»ˆ CER
        final_cer = calculate_cer(gt_text, final_text)
        cer_improvement = original_cer - final_cer

        return SampleResult(
            id=sample_id,
            image_path=image_path,
            gt_text=gt_text,
            agent_a_text=pred_text,
            final_text=final_text,
            original_cer=original_cer,
            final_cer=final_cer,
            cer_improvement=cer_improvement,
            is_vlm_used=is_vlm_used,
            routing_decision=risk_level,
            boundary_risk=boundary_risk,
            boundary_reason=boundary_reason,
            agent_b_text=agent_b_text,
            agent_b_latency_ms=agent_b_latency,
            avg_confidence=avg_conf,
            left_boundary_confidence=left_conf,
            right_boundary_confidence=right_conf,
        )

    def run(
        self, samples: List[Dict], show_progress: bool = True
    ) -> List[SampleResult]:
        """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
        results = []
        self.stats = PipelineStats()
        self.stats.total_samples = len(samples)

        iterator = tqdm(samples, desc="L2W1 Pipeline", miniters=1000) if show_progress else samples

        for sample in iterator:
            try:
                result = self.process_sample(sample)
                results.append(result)

                # æ›´æ–°ç»Ÿè®¡
                self.stats.processed_samples += 1
                self.stats.total_baseline_cer += result.original_cer
                self.stats.total_final_cer += result.final_cer
                self.stats.total_cer_improvement += result.cer_improvement

                if result.is_vlm_used:
                    self.stats.vlm_called_samples += 1
                    self.stats.vlm_total_time_ms += result.agent_b_latency_ms

                if result.boundary_risk:
                    self.stats.boundary_risk_samples += 1
                    if result.cer_improvement > 0:
                        self.stats.boundary_fixed_samples += 1

                if result.routing_decision in self.stats.risk_distribution:
                    self.stats.risk_distribution[result.routing_decision] += 1

                # å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                if show_progress and isinstance(iterator, tqdm):
                    iterator.set_postfix(
                        {
                            "VLM%": f"{self.stats.vlm_call_rate:.1%}",
                            "CERâ†“": f"{self.stats.avg_cer_improvement:.2%}",
                        }
                    )

            except Exception as e:
                print(f"\n[ERROR] å¤„ç†æ ·æœ¬ {sample.get('id', '?')} å¤±è´¥: {e}")
                continue

        return results


# ==================== è¾“å‡ºå‡½æ•° ====================


def save_results(results: List[SampleResult], output_path: Path):
    """ä¿å­˜ç»“æœ"""
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        for result in results:
            result_dict = {
                "id": result.id,
                "image_path": result.image_path,
                "gt_text": result.gt_text,
                "agent_a_text": result.agent_a_text,
                "final_text": result.final_text,
                "original_cer": round(result.original_cer, 4),
                "final_cer": round(result.final_cer, 4),
                "cer_improvement": round(result.cer_improvement, 4),
                "is_vlm_used": result.is_vlm_used,
                "routing_decision": result.routing_decision,
                "boundary_risk": result.boundary_risk,
                "boundary_reason": result.boundary_reason,
                "agent_b_text": result.agent_b_text,
                "agent_b_latency_ms": round(result.agent_b_latency_ms, 2),
                "avg_confidence": round(result.avg_confidence, 4),
                "left_boundary_confidence": round(result.left_boundary_confidence, 4),
                "right_boundary_confidence": round(result.right_boundary_confidence, 4),
            }
            f.write(json.dumps(result_dict, ensure_ascii=False) + "\n")

    print(f"[INFO] ç»“æœå·²ä¿å­˜: {output_path}")


def save_stats_report(stats: PipelineStats, output_path: Path):
    """ä¿å­˜ç»Ÿè®¡æŠ¥å‘Šä¸º JSON"""
    report = {
        "summary": {
            "total_samples": stats.total_samples,
            "processed_samples": stats.processed_samples,
            "vlm_called_samples": stats.vlm_called_samples,
            "vlm_call_rate": round(stats.vlm_call_rate, 4),
        },
        "cer_metrics": {
            "baseline_avg_cer": round(stats.avg_baseline_cer, 4),
            "l2w1_final_avg_cer": round(stats.avg_final_cer, 4),
            "avg_cer_improvement": round(stats.avg_cer_improvement, 4),
            "relative_improvement_percent": round(
                stats.avg_cer_improvement / max(stats.avg_baseline_cer, 0.001) * 100, 2
            ),
        },
        "boundary_analysis": {
            "boundary_risk_samples": stats.boundary_risk_samples,
            "boundary_fixed_samples": stats.boundary_fixed_samples,
            "boundary_fix_rate": round(stats.boundary_fix_rate, 4),
        },
        "latency": {
            "total_time_ms": round(stats.total_time_ms, 2),
            "vlm_total_time_ms": round(stats.vlm_total_time_ms, 2),
            "avg_vlm_latency_ms": round(
                stats.vlm_total_time_ms / max(stats.vlm_called_samples, 1), 2
            ),
        },
        "risk_distribution": stats.risk_distribution,
    }

    report_path = output_path.parent / "l2w1_pipeline_stats.json"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"[INFO] ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def print_summary(stats: PipelineStats):
    """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
    print("\n" + "=" * 70)
    print("L2W1 æµæ°´çº¿æ‰§è¡ŒæŠ¥å‘Š")
    print("=" * 70)

    print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {stats.total_samples}")
    print(f"   å¤„ç†æ ·æœ¬: {stats.processed_samples}")

    print(f"\nğŸ¯ CER æŒ‡æ ‡:")
    print(f"   Baseline å¹³å‡ CER: {stats.avg_baseline_cer:.2%}")
    print(f"   L2W1 æœ€ç»ˆå¹³å‡ CER: {stats.avg_final_cer:.2%}")
    print(f"   å¹³å‡ CER æ”¹è¿›: {stats.avg_cer_improvement:.2%}")
    rel_improve = stats.avg_cer_improvement / max(stats.avg_baseline_cer, 0.001) * 100
    print(f"   ç›¸å¯¹æ”¹è¿›ç‡: {rel_improve:.1f}%")

    print(f"\nğŸ¤– VLM è°ƒç”¨ç»Ÿè®¡:")
    print(f"   VLM è°ƒç”¨æ¬¡æ•°: {stats.vlm_called_samples}")
    print(f"   VLM è°ƒç”¨ç‡: {stats.vlm_call_rate:.1%}")
    print(f"   VLM æ€»è€—æ—¶: {stats.vlm_total_time_ms / 1000:.1f}s")
    if stats.vlm_called_samples > 0:
        avg_latency = stats.vlm_total_time_ms / stats.vlm_called_samples
        print(f"   VLM å¹³å‡å»¶è¿Ÿ: {avg_latency:.0f}ms")

    print(f"\nğŸ”§ è¾¹ç•Œä¿®å¤ç»Ÿè®¡:")
    print(f"   è¾¹ç•Œé£é™©æ ·æœ¬: {stats.boundary_risk_samples}")
    print(f"   è¾¹ç•Œä¿®å¤æˆåŠŸ: {stats.boundary_fixed_samples}")
    print(f"   è¾¹ç•Œä¿®å¤ç‡: {stats.boundary_fix_rate:.1%}")

    print(f"\nğŸ“ˆ é£é™©ç­‰çº§åˆ†å¸ƒ:")
    for level, count in stats.risk_distribution.items():
        ratio = count / max(stats.processed_samples, 1)
        print(f"   {level.upper()}: {count} ({ratio:.1%})")

    # è®ºæ–‡å¯ç”¨å…¬å¼
    print("\n" + "=" * 70)
    print("ğŸ“ è®ºæ–‡å¯ç”¨æ•°æ®:")
    print("=" * 70)
    print(
        f"""
å…¬å¼éªŒè¯:
  CER_baseline = {stats.avg_baseline_cer:.4f}
  CER_l2w1 = {stats.avg_final_cer:.4f}
  CER_improvement = CER_baseline - CER_l2w1 = {stats.avg_cer_improvement:.4f}

å…³é”®æŒ‡æ ‡:
  â€¢ L2W1 å°†å¹³å‡ CER ä» {stats.avg_baseline_cer:.2%} é™ä½åˆ° {stats.avg_final_cer:.2%}
  â€¢ ç›¸å¯¹æ”¹è¿› {rel_improve:.1f}%
  â€¢ VLM è°ƒç”¨ç‡ä»… {stats.vlm_call_rate:.1%}ï¼ˆèŠ‚çœ {(1 - stats.vlm_call_rate) * 100:.1f}% çš„ VLM æˆæœ¬ï¼‰
  â€¢ è¾¹ç•Œä¿®å¤æˆåŠŸç‡ {stats.boundary_fix_rate:.1%}
"""
    )
    print("=" * 70)


# ==================== ä¸»å‡½æ•° ====================


def main():
    parser = argparse.ArgumentParser(
        description="L2W1 å…¨æµç¨‹æ¨ç†ä¸è¯„ä¼° (çœŸå®æ¨¡å¼)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--input",
        type=str,
        default="results/baseline_results.jsonl",
        help="è¾“å…¥ Baseline ç»“æœæ–‡ä»¶",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="results/l2w1_final_results.jsonl",
        help="è¾“å‡ºç»“æœæ–‡ä»¶",
    )
    parser.add_argument(
        "--image_root",
        type=str,
        default="",
        help="å›¾åƒæ ¹ç›®å½• (ç”¨äºè§£æç›¸å¯¹è·¯å¾„)",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="é™åˆ¶å¤„ç†æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰",
    )
    parser.add_argument(
        "--agent_b_model",
        type=str,
        default="Qwen/Qwen2.5-VL-3B-Instruct",
        help="Agent B æ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹ ID",
    )
    parser.add_argument(
        "--no_4bit",
        action="store_true",
        help="ç¦ç”¨ 4-bit é‡åŒ–ï¼ˆéœ€è¦æ›´å¤šæ˜¾å­˜ï¼‰",
    )
    parser.add_argument(
        "--no_flash_attention",
        action="store_true",
        help="ç¦ç”¨ Flash Attention",
    )
    parser.add_argument(
        "--boundary_threshold",
        type=float,
        default=0.8,
        help="è¾¹ç•Œç½®ä¿¡åº¦é˜ˆå€¼",
    )

    args = parser.parse_args()

    print("=" * 70)
    print("L2W1 å…¨æµç¨‹æ¨ç†ä¸è¯„ä¼°è„šæœ¬ (çœŸå®æ¨¡å¼)")
    print("=" * 70)
    print(f"è¾“å…¥æ–‡ä»¶: {args.input}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    print(f"æ ·æœ¬é™åˆ¶: {args.limit if args.limit else 'æ— '}")
    print(f"Agent B æ¨¡å‹: {args.agent_b_model}")
    print(f"4-bit é‡åŒ–: {not args.no_4bit}")
    print(f"Flash Attention: {not args.no_flash_attention}")
    print()

    # æ£€æŸ¥å¿…è¦æ¨¡å—
    if not HAS_AGENT_B:
        print("\n[ERROR] Agent B æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•è¿è¡ŒçœŸå®æ¨¡å¼")
        print("è¯·å®‰è£…ä»¥ä¸‹ä¾èµ–:")
        print("  pip install transformers>=4.37.0 accelerate bitsandbytes")
        print("  pip install flash-attn --no-build-isolation")
        sys.exit(1)

    # åŠ è½½æ•°æ®
    input_path = Path(args.input)
    samples = load_baseline_results(input_path, args.limit)

    if not samples:
        print("[ERROR] æœªåŠ è½½åˆ°æœ‰æ•ˆæ ·æœ¬")
        sys.exit(1)

    # é…ç½®è·¯ç”±å™¨
    router_config = RouterConfig(boundary_confidence_threshold=args.boundary_threshold)

    # åˆå§‹åŒ–æµæ°´çº¿
    print("\n[INFO] åˆå§‹åŒ– L2W1 æµæ°´çº¿...")
    pipeline = L2W1Pipeline(
        router_config=router_config,
        agent_b_model_path=args.agent_b_model,
        use_4bit=not args.no_4bit,
        use_flash_attention=not args.no_flash_attention,
        image_root=args.image_root,
    )

    # è¿è¡Œæµæ°´çº¿
    print("\n[INFO] å¼€å§‹ L2W1 æµæ°´çº¿å¤„ç†...")
    start_time = time.time()

    results = pipeline.run(samples)

    total_time = time.time() - start_time
    pipeline.stats.total_time_ms = total_time * 1000

    # ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    save_results(results, output_path)
    save_stats_report(pipeline.stats, output_path)

    # æ‰“å°æ±‡æ€»
    print_summary(pipeline.stats)

    print(f"\n[INFO] æ€»è€—æ—¶: {total_time:.1f}s")
    print(f"[INFO] å®Œæˆ! è¯·æŸ¥çœ‹ {args.output} è·å–è¯¦ç»†ç»“æœ")


if __name__ == "__main__":
    main()
