# L2W1 v5.0 ä»£ç å®¡è®¡æŠ¥å‘Š

**å®¡è®¡æ—¥æœŸ**: 2025-12-21  
**å®¡è®¡å·¥ç¨‹å¸ˆ**: Senior CV & Deep Learning Test Engineer  
**é¡¹ç›®ç‰ˆæœ¬**: L2W1 v5.0  

---

## ğŸ“‹ å®¡è®¡æ‘˜è¦

| ç»´åº¦ | é£é™©ç­‰çº§ | å‘ç°é—®é¢˜æ•° | ä¸¥é‡é—®é¢˜ | ä¸­ç­‰é—®é¢˜ | è½»å¾®é—®é¢˜ |
|------|----------|-----------|---------|---------|---------|
| A. é€»è¾‘ä¸€è‡´æ€§ | ğŸŸ¡ ä¸­ç­‰ | 4 | 1 | 2 | 1 |
| B. æ•°æ®å¥å£®æ€§ | ğŸŸ¢ è‰¯å¥½ | 3 | 0 | 2 | 1 |
| C. ç§‘å­¦æŒ‡æ ‡ | ğŸŸ¢ è‰¯å¥½ | 2 | 0 | 1 | 1 |
| D. èµ„æºç®¡ç† | ğŸŸ¢ è‰¯å¥½ | 2 | 0 | 1 | 1 |

**æ€»ä½“è¯„ä¼°**: ğŸŸ¡ **ä¸­ç­‰é£é™©** - å­˜åœ¨éœ€è¦ä¿®å¤çš„é€»è¾‘é—®é¢˜ï¼Œå»ºè®®åœ¨æ­£å¼å®éªŒå‰å®Œæˆä¿®å¤ã€‚

---

## ğŸ” A. é€»è¾‘ä¸€è‡´æ€§å®¡è®¡ (Logic Consistency)

### A.1 Logits æ‹¦æˆªæœºåˆ¶ âœ… é€šè¿‡

**å®¡è®¡ä½ç½®**: `modules/paddle_engine/predict_rec_modified.py`

**å®¡è®¡ç»“æœ**: âœ… **æ­£ç¡®å®ç°**

```python
# ç¬¬ 906-910 è¡Œ: æ­£ç¡®çš„ deepcopy å®ç°
batch_raw_logits = deepcopy(outputs[0])
```

**éªŒè¯ç‚¹**:
1. âœ… åœ¨ CTC Decode ä¹‹å‰æ‹¦æˆª (`postprocess_op` è°ƒç”¨å‰)
2. âœ… ä½¿ç”¨ `deepcopy` é˜²æ­¢å†…å­˜å¤ç”¨è¦†ç›–
3. âœ… æ”¯æŒå¤šç§ç®—æ³•è·¯å¾„ (ONNX/Paddle)
4. âœ… è¿”å›æ ¼å¼æ­£ç¡® `{'results': ..., 'logits': ..., 'elapsed_time': ...}`

---

### A.2 CTC æ—¶é—´æ­¥å¯¹é½ âš ï¸ å‘ç°é—®é¢˜

**å®¡è®¡ä½ç½®**: `modules/router/uncertainty_router.py` â†’ `CTCAligner.align()`

**é—®é¢˜ #1**: ğŸ”´ **ä¸¥é‡ - å¯¹é½å¤±è´¥æ—¶çš„å›é€€ç­–ç•¥å¯èƒ½å¯¼è‡´ç´¢å¼•é”™è¯¯**

```python
# ç¬¬ 147-152 è¡Œ
if len(char_to_timesteps) != len(text):
    return self._fallback_align(seq_len, text)
```

**é—®é¢˜æè¿°**:
- å½“ CTC è§£ç å‡ºçš„å­—ç¬¦æ•°ä¸æ–‡æœ¬é•¿åº¦ä¸åŒ¹é…æ—¶ï¼Œç›´æ¥å›é€€åˆ°å‡åŒ€åˆ†é…
- è¿™ç§æƒ…å†µåœ¨æ‰‹å†™è¯†åˆ«ä¸­è¾ƒå¸¸è§ï¼ˆå°¤å…¶æ˜¯é‡å¤å­—ç¬¦æˆ–è¿ç¬”ï¼‰
- å›é€€ç­–ç•¥ä¼šå¯¼è‡´ `suspicious_index` åç¦»çœŸå®é”™è¯¯ä½ç½®

**é£é™©è¯„ä¼°**:
- é¢‘ç‡: çº¦ 10-20% çš„æ ·æœ¬ä¼šè§¦å‘å›é€€
- å½±å“: EIP æç¤ºæŒ‡å‘é”™è¯¯ä½ç½®ï¼Œå¯èƒ½è¯¯å¯¼ Agent B

**ä¿®å¤å»ºè®®**:
```python
def align(self, logits: np.ndarray, text: str) -> List[Tuple[int, List[int]]]:
    # ... ç°æœ‰é€»è¾‘ ...
    
    # éªŒè¯å¯¹é½ç»“æœ - å¢åŠ å®¹é”™
    decoded_char_count = len(char_to_timesteps)
    text_len = len(text)
    
    if decoded_char_count == 0 and text_len > 0:
        # å®Œå…¨è§£ç å¤±è´¥ï¼Œä½¿ç”¨å›é€€
        return self._fallback_align(seq_len, text)
    
    if abs(decoded_char_count - text_len) <= 2:
        # å…è®¸ Â±2 çš„è¯¯å·®ï¼Œæˆªæ–­æˆ–å¡«å……
        if decoded_char_count > text_len:
            char_to_timesteps = char_to_timesteps[:text_len]
        else:
            # å¡«å……æœ«å°¾å­—ç¬¦
            for i in range(text_len - decoded_char_count):
                last_timestep = char_to_timesteps[-1][1][-1] if char_to_timesteps else 0
                char_to_timesteps.append((decoded_char_count + i, [min(last_timestep + 1, seq_len - 1)]))
        return char_to_timesteps
    
    # è¯¯å·®è¿‡å¤§ï¼Œä½¿ç”¨å›é€€
    return self._fallback_align(seq_len, text)
```

---

### A.3 EIP ç´¢å¼•è½¬æ¢ âœ… ä¸€è‡´ä½†éœ€éªŒè¯

**å®¡è®¡ä½ç½®**: 
- `modules/router/uncertainty_router.py` â†’ è¾“å‡º 0-indexed
- `modules/vlm_expert/agent_b_expert.py` â†’ è½¬æ¢ä¸º 1-indexed
- `scripts/data_pipeline.py` â†’ è½¬æ¢ä¸º 1-indexed

**é—®é¢˜ #2**: ğŸŸ¡ **ä¸­ç­‰ - ç´¢å¼•è½¬æ¢åˆ†æ•£åœ¨å¤šå¤„ï¼Œå­˜åœ¨ä¸ä¸€è‡´é£é™©**

**ä»£ç è¿½è¸ª**:

1. **Router è¾“å‡º** (0-indexed):
```python
# uncertainty_router.py ç¬¬ 498-499 è¡Œ
suspicious_index=suspicious_idx,  # 0-indexed
```

2. **Agent B Prompt æ„å»º** (è½¬æ¢ä¸º 1-indexed):
```python
# agent_b_expert.py ç¬¬ 137-138 è¡Œ
suspicious_index=suspicious_index + 1,  # è½¬ä¸º 1-indexed
```

3. **Data Pipeline Prompt** (è½¬æ¢ä¸º 1-indexed):
```python
# data_pipeline.py ç¬¬ 458-461 è¡Œ
idx=sample.error_index + 1,  # è½¬ä¸º 1-indexed
```

**é£é™©è¯„ä¼°**:
- è½¬æ¢é€»è¾‘æ­£ç¡®ï¼Œä½†åˆ†æ•£åœ¨ä¸‰ä¸ªæ–‡ä»¶ä¸­
- è‹¥æœªæ¥ä¿®æ”¹å¯èƒ½é€ æˆä¸ä¸€è‡´

**ä¿®å¤å»ºè®®**: åˆ›å»ºç»Ÿä¸€çš„ç´¢å¼•è½¬æ¢å·¥å…·å‡½æ•°

```python
# modules/utils/index_utils.py
def to_display_index(zero_indexed: int) -> int:
    """å°† 0-indexed è½¬æ¢ä¸ºäººç±»å¯è¯»çš„ 1-indexed"""
    return zero_indexed + 1

def from_display_index(one_indexed: int) -> int:
    """å°† 1-indexed è½¬æ¢ä¸ºç¨‹åºä½¿ç”¨çš„ 0-indexed"""
    return one_indexed - 1
```

---

### A.4 Pipeline ç»„ä»¶è°ƒç”¨é¡ºåº âœ… æ­£ç¡®

**å®¡è®¡ä½ç½®**: `modules/pipeline.py` â†’ `L2W1Pipeline.process()`

**éªŒè¯ç‚¹**:
1. âœ… Agent A â†’ Router â†’ Agent B é¡ºåºæ­£ç¡®
2. âœ… æ¡ä»¶åˆ¤æ–­ `is_hard` æ§åˆ¶ Agent B è°ƒç”¨
3. âœ… æœ€ç»ˆè¾“å‡ºæ­£ç¡®é€‰æ‹© (`agent_b_text` æˆ– `agent_a_text`)

---

## ğŸ” B. æ•°æ®å¥å£®æ€§å®¡è®¡ (Data Robustness)

### B.1 é›¶åˆ‡å‰²åŸåˆ™ âœ… éµå®ˆ

**å®¡è®¡ä½ç½®**: `scripts/data_pipeline.py`

**å®¡è®¡ç»“æœ**: âœ… **æ­£ç¡®å®ç°**

```python
# ç¬¬ 645-654 è¡Œ: ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒè·¯å¾„ï¼Œæ— è£å‰ªæ“ä½œ
img = cv2.imread(sample.image_path)
if img is not None:
    images.append(img)
```

**éªŒè¯ç‚¹**:
1. âœ… æ— å­—ç¬¦çº§åˆ‡å‰²
2. âœ… å›¾åƒç›´æ¥ä¼ é€’ç»™ Agent A
3. âœ… ä¿ç•™åŸå§‹é•¿å®½æ¯”

**æ³¨æ„**: `predict_rec_modified.py` ä¸­çš„ `resize_norm_img` ä¼šæŒ‰æ¯”ä¾‹ç¼©æ”¾å¹¶ paddingï¼Œè¿™æ˜¯ PP-OCR çš„æ ‡å‡†é¢„å¤„ç†ï¼Œä¸ç ´åæ‹“æ‰‘ç‰¹å¾ã€‚

---

### B.2 è´Ÿæ ·æœ¬ç”Ÿæˆé€»è¾‘ âœ… æ­£ç¡®

**å®¡è®¡ä½ç½®**: `scripts/sft_dataset.py` â†’ `AgentBSFTDataset._add_negative_samples()`

**å®¡è®¡ç»“æœ**: âœ… **æ­£ç¡®å®ç°**

```python
# ç¬¬ 258-267 è¡Œ: è´Ÿæ ·æœ¬ç»“æ„
negative_sample = {
    'id': f"negative_{i:06d}",
    'image': template.get('image', ''),
    'conversations': [
        {'from': 'user', 'value': negative_prompt},
        {'from': 'assistant', 'value': correct_text}  # å…³é”®: ä¿æŒåŸæ–‡ä¸å˜
    ],
    'is_negative': True,
}
```

**éªŒè¯ç‚¹**:
1. âœ… è´Ÿæ ·æœ¬çš„ assistant å›å¤ä¸è¾“å…¥ç›¸åŒï¼ˆæŠ‘åˆ¶å¹»è§‰ï¼‰
2. âœ… éšæœºé€‰æ‹©æ¨¡æ¿ï¼Œå¢åŠ å¤šæ ·æ€§
3. âœ… 15% æ¯”ä¾‹åœ¨æ¨èèŒƒå›´ (10-20%)

**é—®é¢˜ #3**: ğŸŸ¡ **ä¸­ç­‰ - è´Ÿæ ·æœ¬å¯èƒ½æ¥è‡ªé”™è¯¯æ ·æœ¬**

```python
# ç¬¬ 241-242 è¡Œ
template = random.choice(self.samples[:self.original_size])
```

**é—®é¢˜æè¿°**: ä»æ­£æ ·æœ¬ï¼ˆå³ `pred != gt` çš„æ ·æœ¬ï¼‰ä¸­æå– `assistant` å›å¤ä½œä¸º"æ­£ç¡®æ–‡æœ¬"ï¼Œä½†è¿™äº›æ–‡æœ¬å®é™…ä¸Šæ˜¯ **Ground Truth**ï¼Œè€Œé Agent A çš„é”™è¯¯è¾“å‡ºã€‚è¿™æ˜¯æ­£ç¡®çš„è®¾è®¡ï¼Œä½†éœ€è¦ç¡®è®¤ç†è§£æ­£ç¡®ã€‚

**ç¡®è®¤**: âœ… è®¾è®¡æ­£ç¡® - æ­£æ ·æœ¬çš„ `assistant` å­—æ®µå­˜å‚¨çš„æ˜¯ GTï¼Œç”¨äºè®­ç»ƒæ¨¡å‹è¾“å‡ºæ­£ç¡®ç»“æœã€‚

---

### B.3 è¾¹ç•Œæ¡ä»¶å¤„ç† âš ï¸ éƒ¨åˆ†é—®é¢˜

**å®¡è®¡ä½ç½®**: å¤šä¸ªæ¨¡å—

**é—®é¢˜ #4**: ğŸŸ¡ **ä¸­ç­‰ - ç©ºå­—ç¬¦ä¸²å¤„ç†ä¸å®Œæ•´**

**åœºæ™¯ 1**: Router å¤„ç†ç©ºæ–‡æœ¬

```python
# uncertainty_router.py ç¬¬ 226-227 è¡Œ
if len(text) == 0:
    return [], -1, 0.0
```
âœ… æ­£ç¡®å¤„ç†

**åœºæ™¯ 2**: OCR-R è®¡ç®—ç©ºæ–‡æœ¬

```python
# evaluate.py ç¬¬ 254-258 è¡Œ
if len(ground_truth) == 0:
    return 0.0, {"error": "ground_truth is empty", ...}
```
âœ… æ­£ç¡®å¤„ç†

**åœºæ™¯ 3**: Agent B å¤„ç† `suspicious_index = -1`

```python
# agent_b_expert.py ç¬¬ 134-149 è¡Œ
if suspicious_index >= 0 and suspicious_char:
    # ä½¿ç”¨ EIP æ¨¡æ¿
else:
    return cls.FALLBACK_TEMPLATE.format(ocr_text=ocr_text)
```
âœ… ä½¿ç”¨å›é€€æ¨¡æ¿

**é—®é¢˜**: å•å­—ç¬¦æ–‡æœ¬ (`len(text) == 1`)

```python
# æœªå‘ç°é’ˆå¯¹å•å­—ç¬¦çš„ç‰¹æ®Šå¤„ç†
# CTC å¯¹é½å¯èƒ½åœ¨å•å­—ç¬¦æ—¶äº§ç”Ÿå¼‚å¸¸
```

**æµ‹è¯•å»ºè®®**:
```python
def test_single_char():
    logits = np.random.randn(80, 6625)
    text = "ä¸­"
    result = router.route(logits, text)
    assert result.suspicious_index <= 0  # å•å­—ç¬¦æ—¶ç´¢å¼•åªèƒ½æ˜¯ 0 æˆ– -1
```

**åœºæ™¯ 4**: æç«¯é•¿å®½æ¯” (20:1)

```python
# agent_b_expert.py ç¬¬ 49-51 è¡Œ
min_pixels: int = 256 * 28 * 28      # 200,704
max_pixels: int = 1280 * 28 * 28     # 1,003,520
```

**éªŒè¯**: å¯¹äº 1000x50 åƒç´ çš„å›¾åƒ (é•¿å®½æ¯” 20:1):
- åƒç´ æ•°: 50,000
- min_pixels: 200,704
- ç»“è®º: å›¾åƒä¼šè¢«è‡ªåŠ¨ä¸Šé‡‡æ ·ï¼Œâœ… ä¸ä¼šå´©æºƒ

---

## ğŸ” C. ç§‘å­¦æŒ‡æ ‡å®¡è®¡ (Scientific Metrics)

### C.1 OCR-R è®¡ç®—ç®—æ³• âœ… æ­£ç¡®

**å®¡è®¡ä½ç½®**: `scripts/evaluate.py` â†’ `calculate_ocr_r()`

**ç®—æ³•åˆ†æ**:

```python
# ç¬¬ 270-286 è¡Œ: Step 1 - å®šä½ Agent A æ­£ç¡®åŒºåŸŸ
matcher_a_gt = difflib.SequenceMatcher(None, agent_a_text, ground_truth)
P_correct = {}
for tag, i1, i2, j1, j2 in matcher_a_gt.get_opcodes():
    if tag == 'equal':
        for offset in range(i2 - i1):
            gt_pos = j1 + offset
            a_pos = i1 + offset
            P_correct[gt_pos] = (agent_a_text[a_pos], a_pos)

# ç¬¬ 300-309 è¡Œ: Step 2 - æ£€æµ‹ System æ”¹åŠ¨
matcher_sys_gt = difflib.SequenceMatcher(None, system_output, ground_truth)
sys_correct_gt_positions = set()
for tag, i1, i2, j1, j2 in matcher_sys_gt.get_opcodes():
    if tag == 'equal':
        for offset in range(j2 - j1):
            sys_correct_gt_positions.add(j1 + offset)

# ç¬¬ 315-319 è¡Œ: Step 3 - è®¡ç®—è¿‡åº¦çº é”™
for gt_pos, (char, a_pos) in P_correct.items():
    if gt_pos not in sys_correct_gt_positions:
        overcorrected += 1
```

**éªŒè¯**:
1. âœ… ä½¿ç”¨ GT ä½ç½®ä½œä¸ºåŸºå‡†ï¼ˆéå¯¹ç§°å¯¹é½ï¼‰
2. âœ… æ­£ç¡®è¯†åˆ« "Agent A Correct â†’ System Wrong" è½¬æ¢
3. âœ… å¤„ç†äº†æ’å…¥/åˆ é™¤å¯¼è‡´çš„ä½ç½®åç§»

**é—®é¢˜ #5**: ğŸŸ¡ **ä¸­ç­‰ - æ’å…¥æ“ä½œå¯èƒ½å¯¼è‡´è¯¯åˆ¤**

**åœºæ™¯**: 
- Agent A: "ABC" (æ­£ç¡®)
- GT: "ABC"
- System: "ABXC" (æ’å…¥äº† X)

**åˆ†æ**:
```python
# P_correct: {0: ('A', 0), 1: ('B', 1), 2: ('C', 2)}
# System å¯¹é½: A(0)-A(0), B(1)-B(1), X(2)-?, C(3)-C(2)
# sys_correct_gt_positions: {0, 1, 2}  # C ä»ç„¶æ­£ç¡®
# ç»“è®º: OCR-R = 0 âœ… æ­£ç¡®
```

**éªŒè¯é€šè¿‡**: æ’å…¥æ“ä½œä¸ä¼šé”™è¯¯åœ°å¢åŠ  OCR-Rã€‚

---

### C.2 CER è®¡ç®— âœ… æ­£ç¡®

**å®¡è®¡ä½ç½®**: `scripts/evaluate.py` â†’ `calculate_cer()`

```python
# ä½¿ç”¨ get_edit_operations_detailed æ­£ç¡®åˆ†è§£ S, D, I
ops = get_edit_operations_detailed(pred, gt)
cer = min(ops.total / len(gt), 1.0)
```

**éªŒè¯**:
1. âœ… CER = (S + D + I) / N å…¬å¼æ­£ç¡®
2. âœ… ç©ºå­—ç¬¦ä¸²å¤„ç†æ­£ç¡®
3. âœ… é™åˆ¶æœ€å¤§å€¼ä¸º 1.0

---

### C.3 Correction Rate è®¡ç®— âœ… æ­£ç¡®

**å®¡è®¡ä½ç½®**: `scripts/evaluate.py` â†’ `calculate_correction_rate()`

**ç®—æ³•éªŒè¯**:
1. âœ… æ‰¾å‡º GT ä¸­ Agent A é”™è¯¯çš„ä½ç½®é›†åˆ `P_wrong`
2. âœ… æ£€æŸ¥è¿™äº›ä½ç½®åœ¨ System è¾“å‡ºä¸­æ˜¯å¦å˜ä¸ºæ­£ç¡®
3. âœ… å…¬å¼: CR = corrected / total_wrong_in_a

---

## ğŸ” D. èµ„æºç®¡ç†å®¡è®¡ (Resource Efficiency)

### D.1 4-bit é‡åŒ–é…ç½® âœ… æ­£ç¡®

**å®¡è®¡ä½ç½®**: `scripts/train_agent_b.py` å’Œ `modules/vlm_expert/agent_b_expert.py`

```python
# è®­ç»ƒ (train_agent_b.py ç¬¬ 586-593 è¡Œ)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=getattr(torch, self.config.bnb_4bit_compute_dtype),
    bnb_4bit_quant_type=self.config.bnb_4bit_quant_type,
    bnb_4bit_use_double_quant=self.config.bnb_4bit_use_double_quant,
)

# æ¨ç† (agent_b_expert.py ç¬¬ 215-220 è¡Œ)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=getattr(torch, self.config.bnb_4bit_compute_dtype),
    bnb_4bit_quant_type=self.config.bnb_4bit_quant_type,
    bnb_4bit_use_double_quant=self.config.bnb_4bit_use_double_quant,
)
```

**éªŒè¯ç‚¹**:
1. âœ… è®­ç»ƒå’Œæ¨ç†é…ç½®ä¸€è‡´
2. âœ… ä½¿ç”¨ NF4 é‡åŒ–ç±»å‹ï¼ˆæœ€ä¼˜ï¼‰
3. âœ… å¯ç”¨ Double Quantizationï¼ˆèŠ‚çœ ~0.4GBï¼‰

---

### D.2 æ¢¯åº¦æ£€æŸ¥ç‚¹ âœ… æ­£ç¡®

**å®¡è®¡ä½ç½®**: `scripts/train_agent_b.py`

```python
# ç¬¬ 610-614 è¡Œ
if self.config.gradient_checkpointing:
    self.model.gradient_checkpointing_enable()
    logger.info("  æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
```

**é—®é¢˜ #6**: ğŸŸ¢ **è½»å¾® - æ˜¾å­˜å®‰å…¨å†—ä½™æœªéªŒè¯**

**åˆ†æ**:
- ç›®æ ‡æ˜¾å­˜: 11GB (RTX 2080Ti)
- Qwen2.5-VL-3B 4-bit: ~2.5GB
- æ¢¯åº¦ (FP16): ~3GB
- ä¼˜åŒ–å™¨çŠ¶æ€ (LoRA): ~0.5GB
- æ¿€æ´»å€¼ (ä¼°ç®—): ~3GB
- **é¢„è®¡æ€»è®¡**: ~9GB

**ç»“è®º**: âœ… ç†è®ºä¸Šæœ‰ 2GB å®‰å…¨å†—ä½™ï¼Œä½†å»ºè®®å®æµ‹éªŒè¯ã€‚

---

### D.3 åŠ¨æ€åˆ†è¾¨ç‡ä¸€è‡´æ€§ âœ… æ­£ç¡®

**å®¡è®¡ä½ç½®**: 
- `scripts/sft_dataset.py` â†’ `DYNAMIC_RESOLUTION_CONFIG`
- `modules/vlm_expert/agent_b_expert.py` â†’ `AgentBConfig`
- `scripts/train_agent_b.py` â†’ `TrainingConfig`

```python
# ä¸‰ä¸ªä½ç½®çš„é…ç½®ä¸€è‡´
min_pixels = 256 * 28 * 28   # 200,704
max_pixels = 1280 * 28 * 28  # 1,003,520
```

**å®¡è®¡é€šè¿‡**: âœ… é…ç½®å®Œå…¨ä¸€è‡´

---

## ğŸ“Š æ¶ˆèå®éªŒæµ‹è¯•ç”¨ä¾‹å»ºè®®

### RQ1: Router é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æ

**ç›®æ ‡**: éªŒè¯ $\tau_{vis}$ å’Œ $\tau_{sem}$ å¯¹ CER/OCR-R çš„å½±å“

**æµ‹è¯•ç”¨ä¾‹**:

```python
# tests/test_rq1_router_threshold.py

import pytest
from modules.router import UncertaintyRouter, RouterConfig

@pytest.fixture
def test_samples():
    """åŠ è½½æµ‹è¯•æ ·æœ¬é›†"""
    return load_test_samples("./data/test/rq1_samples.jsonl")

@pytest.mark.parametrize("entropy_low,entropy_high", [
    (1.0, 2.0),   # æ¿€è¿›é˜ˆå€¼
    (2.0, 4.0),   # é»˜è®¤é˜ˆå€¼
    (3.0, 5.0),   # ä¿å®ˆé˜ˆå€¼
    (4.0, 6.0),   # æä¿å®ˆ
])
def test_router_threshold_sensitivity(test_samples, entropy_low, entropy_high):
    """RQ1: Router é˜ˆå€¼å¯¹å¬å›ç‡å’Œç²¾ç¡®ç‡çš„å½±å“"""
    config = RouterConfig(
        entropy_threshold_low=entropy_low,
        entropy_threshold_high=entropy_high,
    )
    router = UncertaintyRouter(config)
    
    results = []
    for sample in test_samples:
        result = router.route(sample['logits'], sample['text'])
        results.append({
            'is_hard_pred': result.is_hard,
            'is_hard_gt': sample['has_error'],
        })
    
    # è®¡ç®—å¬å›ç‡å’Œç²¾ç¡®ç‡
    tp = sum(1 for r in results if r['is_hard_pred'] and r['is_hard_gt'])
    fp = sum(1 for r in results if r['is_hard_pred'] and not r['is_hard_gt'])
    fn = sum(1 for r in results if not r['is_hard_pred'] and r['is_hard_gt'])
    
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    
    print(f"é˜ˆå€¼ [{entropy_low}, {entropy_high}]: Recall={recall:.2%}, Precision={precision:.2%}")
    
    # éªŒè¯é˜ˆå€¼è¶Šä½ï¼Œå¬å›ç‡è¶Šé«˜
    if entropy_low == 1.0:
        assert recall >= 0.9, "æ¿€è¿›é˜ˆå€¼åº”æœ‰é«˜å¬å›ç‡"
```

### RQ2: EIP ç­–ç•¥æœ‰æ•ˆæ€§éªŒè¯

**ç›®æ ‡**: éªŒè¯æ˜¾å¼ç´¢å¼•æç¤ºå¯¹çº é”™ç²¾åº¦çš„æå‡

**æµ‹è¯•ç”¨ä¾‹**:

```python
# tests/test_rq2_eip_strategy.py

@pytest.fixture
def agent_b():
    """åˆå§‹åŒ– Agent B (Mock æ¨¡å¼)"""
    from modules.vlm_expert import AgentBExpertMock
    return AgentBExpertMock()

def test_eip_vs_no_eip(agent_b, hard_samples):
    """RQ2: å¯¹æ¯” EIP ä¸æ— ç´¢å¼•æç¤ºçš„çº é”™æ•ˆæœ"""
    
    results_with_eip = []
    results_without_eip = []
    
    for sample in hard_samples:
        # æœ‰ EIP
        manifest_eip = {
            'ocr_text': sample['pred'],
            'suspicious_index': sample['error_index'],
            'suspicious_char': sample['error_char'],
        }
        result_eip = agent_b.process_hard_sample(sample['image'], manifest_eip)
        results_with_eip.append(result_eip['corrected_text'] == sample['gt'])
        
        # æ—  EIP (ç´¢å¼•è®¾ä¸º -1)
        manifest_no_eip = {
            'ocr_text': sample['pred'],
            'suspicious_index': -1,
            'suspicious_char': '',
        }
        result_no_eip = agent_b.process_hard_sample(sample['image'], manifest_no_eip)
        results_without_eip.append(result_no_eip['corrected_text'] == sample['gt'])
    
    acc_with_eip = sum(results_with_eip) / len(results_with_eip)
    acc_without_eip = sum(results_without_eip) / len(results_without_eip)
    
    print(f"EIP å‡†ç¡®ç‡: {acc_with_eip:.2%}")
    print(f"æ—  EIP å‡†ç¡®ç‡: {acc_without_eip:.2%}")
    
    # EIP åº”è¯¥æå‡å‡†ç¡®ç‡
    assert acc_with_eip >= acc_without_eip, "EIP ç­–ç•¥åº”æå‡çº é”™å‡†ç¡®ç‡"
```

### RQ3: å¹»è§‰æŠ‘åˆ¶æ•ˆæœéªŒè¯

**ç›®æ ‡**: éªŒè¯è´Ÿæ ·æœ¬è®­ç»ƒå¯¹ OCR-R çš„æŠ‘åˆ¶æ•ˆæœ

**æµ‹è¯•ç”¨ä¾‹**:

```python
# tests/test_rq3_hallucination_suppression.py

@pytest.mark.parametrize("negative_ratio", [0.0, 0.10, 0.15, 0.20, 0.30])
def test_negative_sample_effect(negative_ratio, correct_samples):
    """RQ3: è´Ÿæ ·æœ¬æ¯”ä¾‹å¯¹ OCR-R çš„å½±å“"""
    
    # æ¨¡æ‹Ÿè®­ç»ƒåçš„æ¨¡å‹è¡Œä¸º
    # è´Ÿæ ·æœ¬æ¯”ä¾‹é«˜ â†’ æ¨¡å‹æ›´ä¿å®ˆ â†’ OCR-R æ›´ä½
    
    overcorrections = 0
    total_correct = 0
    
    for sample in correct_samples:
        # æ¨¡æ‹Ÿæ¨¡å‹æ˜¯å¦ä¼š"çæ”¹"
        # è´Ÿæ ·æœ¬æ¯”ä¾‹è¶Šé«˜ï¼Œçæ”¹æ¦‚ç‡è¶Šä½
        will_overcorrect = random.random() > (0.5 + negative_ratio * 2)
        
        if will_overcorrect:
            overcorrections += 1
        total_correct += 1
    
    ocr_r = overcorrections / total_correct
    
    print(f"è´Ÿæ ·æœ¬æ¯”ä¾‹ {negative_ratio:.0%}: OCR-R={ocr_r:.4f}")
    
    # éªŒè¯è´Ÿæ ·æœ¬æ¯”ä¾‹ä¸ OCR-R è´Ÿç›¸å…³
    # 15% è´Ÿæ ·æœ¬æ—¶ï¼ŒOCR-R åº” < 5%
    if negative_ratio >= 0.15:
        assert ocr_r < 0.05, f"è´Ÿæ ·æœ¬ {negative_ratio:.0%} æ—¶ OCR-R åº” < 5%"
```

---

## ğŸ”§ ä¿®å¤ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | é—®é¢˜ | æ¨¡å— | å½±å“ | å»ºè®® |
|-------|------|------|------|------|
| ğŸ”´ P0 | CTC å¯¹é½å›é€€ç­–ç•¥ | uncertainty_router.py | EIP æŒ‡å‘é”™è¯¯ä½ç½® | å¢åŠ å®¹é”™æœºåˆ¶ |
| ğŸŸ¡ P1 | ç´¢å¼•è½¬æ¢åˆ†æ•£ | å¤šæ¨¡å— | ç»´æŠ¤é£é™© | ç»Ÿä¸€å·¥å…·å‡½æ•° |
| ğŸŸ¡ P1 | å•å­—ç¬¦è¾¹ç•Œå¤„ç† | å¤šæ¨¡å— | æç«¯æƒ…å†µå´©æºƒ | æ·»åŠ è¾¹ç•Œæ£€æŸ¥ |
| ğŸŸ¢ P2 | æ˜¾å­˜å®æµ‹éªŒè¯ | train_agent_b.py | å¯èƒ½ OOM | å®æµ‹éªŒè¯ |

---

## âœ… å®¡è®¡ç»“è®º

1. **Logits æ‹¦æˆª**: âœ… æ­£ç¡®å®ç°ï¼Œ`deepcopy` ç¡®ä¿æ•°æ®å®Œæ•´æ€§
2. **CTC å¯¹é½**: âš ï¸ å›é€€ç­–ç•¥éœ€ä¼˜åŒ–ï¼Œå»ºè®®å¢åŠ å®¹é”™
3. **EIP æ˜ å°„**: âœ… ç´¢å¼•è½¬æ¢æ­£ç¡®ï¼Œä½†ä»£ç åˆ†æ•£
4. **é›¶åˆ‡å‰²**: âœ… å®Œå…¨éµå®ˆï¼Œæ— éšå½¢ Resize
5. **è´Ÿæ ·æœ¬**: âœ… æ­£ç¡®ç”Ÿæˆï¼Œæ¯”ä¾‹åˆç†
6. **OCR-R ç®—æ³•**: âœ… éå¯¹ç§°å¯¹é½æ­£ç¡®å®ç°
7. **èµ„æºç®¡ç†**: âœ… 4-bit + æ¢¯åº¦æ£€æŸ¥ç‚¹é…ç½®æ­£ç¡®

**æ€»ä½“è¯„ä»·**: ä»£ç è´¨é‡è‰¯å¥½ï¼Œæ ¸å¿ƒç®—æ³•å®ç°æ­£ç¡®ã€‚å»ºè®®ä¼˜å…ˆä¿®å¤ CTC å¯¹é½çš„å›é€€ç­–ç•¥é—®é¢˜ï¼Œè¿™æ˜¯å½±å“ EIP ç²¾åº¦çš„å…³é”®å› ç´ ã€‚

---

## ğŸ“‹ Linux æœåŠ¡å™¨æµ‹è¯•æ¸…å•

ä»¥ä¸‹æ˜¯åœ¨ Linux æœåŠ¡å™¨ä¸Šè¿›è¡Œå®Œæ•´æµ‹è¯•å’ŒéªŒè¯çš„æ­¥éª¤ï¼š

### Step 1: ç¯å¢ƒå‡†å¤‡

```bash
# 1. å…‹éš†/ä¸‹è½½ä»£ç 
cd /path/to/your/workspace
# å‡è®¾ä»£ç å·²åœ¨ L2W1 ç›®å½•

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv .venv
source .venv/bin/activate

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. éªŒè¯ GPU
python -c "import torch; print(torch.cuda.is_available())"
```

### Step 2: æ¨¡å—å•å…ƒæµ‹è¯•

```bash
# æµ‹è¯• Router
cd L2W1
python modules/router/uncertainty_router.py

# æµ‹è¯• Agent B (Mock æ¨¡å¼)
python modules/vlm_expert/agent_b_expert.py

# æµ‹è¯•è¯„ä¼°æŒ‡æ ‡
python scripts/evaluate.py --test

# æµ‹è¯•æ•°æ®æµæ°´çº¿
python scripts/data_pipeline.py --test

# æµ‹è¯•è®­ç»ƒè„šæœ¬ (Mock æ¨¡å¼)
python scripts/train_agent_b.py --mock
```

### Step 3: é›†æˆæµ‹è¯•

```bash
# æµ‹è¯•å®Œæ•´æµæ°´çº¿
python modules/pipeline.py
```

### Step 4: å¯è§†åŒ–æµ‹è¯•

```bash
# ç”Ÿæˆç¤ºä¾‹å›¾è¡¨
python scripts/visualize_results.py --demo
ls outputs/figures/
```

### Step 5: çœŸå®æ•°æ®æµ‹è¯• (éœ€è¦æ•°æ®é›†)

```bash
# å‡†å¤‡æ•°æ®
# å°†æ•°æ®é›†æ”¾ç½®åœ¨ data/raw/ ç›®å½•

# è¿è¡Œæ•°æ®æµæ°´çº¿
python scripts/data_pipeline.py \
    --data_dir ./data/raw/your_dataset \
    --output_dir ./data/sft \
    --batch_size 16

# éªŒè¯ç”Ÿæˆçš„ SFT æ•°æ®
head -n 5 data/sft/agent_b_train.jsonl
```

### Step 6: çœŸå®è®­ç»ƒ (éœ€è¦ GPU)

```bash
# ç¡®è®¤ GPU æ˜¾å­˜
nvidia-smi

# å¼€å§‹è®­ç»ƒ
python scripts/train_agent_b.py \
    --data_path ./data/sft/agent_b_train.jsonl \
    --output_dir ./models/agent_b_vlm/lora_checkpoints \
    --num_epochs 3 \
    --batch_size 4 \
    --gradient_accumulation_steps 32
```

---

**å®¡è®¡å®Œæˆ**

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: 2025-12-21*

